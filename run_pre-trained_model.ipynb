{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import joblib\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from classification_model.opt import adam, warmup_cosine, warmup_linear, warmup_constant\n",
    "#from classification_model.analysis import rocstories as rocstories_analysis\n",
    "from classification_model.text_utils import TextEncoder\n",
    "from classification_model.utils import encode_dataset, flatten, iter_data, find_trainable_variables, convert_gradient_to_tensor, shape_list, ResultLogger, assign_to_gpu, average_grads, make_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = 'entailment_test.csv'\n",
    "test_dataset = 'cluster_0_news_unprocessed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5047 5047 5047 10 10 10 4086 4086\n",
      "40479\n",
      "256\n",
      "254\n",
      "(5047, 2, 256, 2) (5047, 2, 256) (10, 2, 256, 2) (10, 2, 256) (4086, 2, 256, 2) (4086, 2, 256)\n",
      "5047 10 32 471\n",
      "(?, 2, 256, 2)\n",
      "(?, 2, 256)\n",
      "(32, 2, 256, 2)\n",
      "(32, 2, 256)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "def gelu(x):\n",
    "    return 0.5*x*(1+tf.tanh(math.sqrt(2/math.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "def swish(x):\n",
    "    return x*tf.nn.sigmoid(x)\n",
    "\n",
    "opt_fns = {\n",
    "    'adam':adam,\n",
    "}\n",
    "\n",
    "act_fns = {\n",
    "    'relu':tf.nn.relu,\n",
    "    'swish':swish,\n",
    "    'gelu':gelu\n",
    "}\n",
    "\n",
    "lr_schedules = {\n",
    "    'warmup_cosine':warmup_cosine,\n",
    "    'warmup_linear':warmup_linear,\n",
    "    'warmup_constant':warmup_constant,\n",
    "}\n",
    "\n",
    "def _norm(x, g=None, b=None, e=1e-5, axis=[1]):\n",
    "    u = tf.reduce_mean(x, axis=axis, keep_dims=True)\n",
    "    s = tf.reduce_mean(tf.square(x-u), axis=axis, keep_dims=True)\n",
    "    x = (x - u) * tf.rsqrt(s + e)\n",
    "    if g is not None and b is not None:\n",
    "        x = x*g + b\n",
    "    return x\n",
    "\n",
    "def norm(x, scope, axis=[-1]):\n",
    "    with tf.variable_scope(scope):\n",
    "        n_state = shape_list(x)[-1]\n",
    "        g = tf.get_variable(\"g\", [n_state], initializer=tf.constant_initializer(1))\n",
    "        b = tf.get_variable(\"b\", [n_state], initializer=tf.constant_initializer(0))\n",
    "        return _norm(x, g, b, axis=axis)\n",
    "\n",
    "def dropout(x, pdrop, train):\n",
    "    if train and pdrop > 0:\n",
    "        x = tf.nn.dropout(x, 1-pdrop)\n",
    "    return x\n",
    "\n",
    "def mask_attn_weights(w):\n",
    "    n = shape_list(w)[-1]\n",
    "    b = tf.matrix_band_part(tf.ones([n, n]), -1, 0)\n",
    "    b = tf.reshape(b, [1, 1, n, n])\n",
    "    w = w*b + -1e9*(1-b)\n",
    "    return w\n",
    "\n",
    "def _attn(q, k, v, train=False, scale=False):\n",
    "    w = tf.matmul(q, k)\n",
    "\n",
    "    if scale:\n",
    "        n_state = shape_list(v)[-1]\n",
    "        w = w*tf.rsqrt(tf.cast(n_state, tf.float32))\n",
    "\n",
    "    w = mask_attn_weights(w)\n",
    "    w = tf.nn.softmax(w)\n",
    "\n",
    "    w = dropout(w, attn_pdrop, train)\n",
    "\n",
    "    a = tf.matmul(w, v)\n",
    "    return a\n",
    "\n",
    "def split_states(x, n):\n",
    "    x_shape = shape_list(x)\n",
    "    m = x_shape[-1]\n",
    "    new_x_shape = x_shape[:-1]+[n, m//n]\n",
    "    return tf.reshape(x, new_x_shape)\n",
    "\n",
    "def merge_states(x):\n",
    "    x_shape = shape_list(x)\n",
    "    new_x_shape = x_shape[:-2]+[np.prod(x_shape[-2:])]\n",
    "    return tf.reshape(x, new_x_shape)\n",
    "\n",
    "def split_heads(x, n, k=False):\n",
    "    if k:\n",
    "        return tf.transpose(split_states(x, n), [0, 2, 3, 1])\n",
    "    else:\n",
    "        return tf.transpose(split_states(x, n), [0, 2, 1, 3])\n",
    "\n",
    "def merge_heads(x):\n",
    "    return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "def conv1d(x, scope, nf, rf, w_init=tf.random_normal_initializer(stddev=0.02), b_init=tf.constant_initializer(0), pad='VALID', train=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        w = tf.get_variable(\"w\", [rf, nx, nf], initializer=w_init)\n",
    "        b = tf.get_variable(\"b\", [nf], initializer=b_init)\n",
    "        if rf == 1: #faster 1x1 conv\n",
    "            c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, shape_list(x)[:-1]+[nf])\n",
    "        else: #was used to train LM\n",
    "            c = tf.nn.conv1d(x, w, stride=1, padding=pad)+b\n",
    "        return c\n",
    "\n",
    "def attn(x, scope, n_state, n_head, train=False, scale=False):\n",
    "    assert n_state%n_head==0\n",
    "    with tf.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3, 1, train=train)\n",
    "        q, k, v = tf.split(c, 3, 2)\n",
    "        q = split_heads(q, n_head)\n",
    "        k = split_heads(k, n_head, k=True)\n",
    "        v = split_heads(v, n_head)\n",
    "        a = _attn(q, k, v, train=train, scale=scale)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state, 1, train=train)\n",
    "        a = dropout(a, resid_pdrop, train)\n",
    "        return a\n",
    "\n",
    "def mlp(x, scope, n_state, train=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        act = act_fns[afn]\n",
    "        h = act(conv1d(x, 'c_fc', n_state, 1, train=train))\n",
    "        h2 = conv1d(h, 'c_proj', nx, 1, train=train)\n",
    "        h2 = dropout(h2, resid_pdrop, train)\n",
    "        return h2\n",
    "\n",
    "def block(x, scope, train=False, scale=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = shape_list(x)[-1]\n",
    "        a = attn(x, 'attn', nx, n_head, train=train, scale=scale)\n",
    "        n = norm(x+a, 'ln_1')\n",
    "        m = mlp(n, 'mlp', nx*4, train=train)\n",
    "        h = norm(n+m, 'ln_2')\n",
    "        return h\n",
    "\n",
    "def embed(X, we):\n",
    "    we = convert_gradient_to_tensor(we)\n",
    "    e = tf.gather(we, X)\n",
    "    h = tf.reduce_sum(e, 2)\n",
    "    return h\n",
    "\n",
    "def clf(x, ny, w_init=tf.random_normal_initializer(stddev=0.02), b_init=tf.constant_initializer(0), train=False):\n",
    "    with tf.variable_scope('clf'):\n",
    "        nx = shape_list(x)[-1]\n",
    "        w = tf.get_variable(\"w\", [nx, ny], initializer=w_init)\n",
    "        b = tf.get_variable(\"b\", [ny], initializer=b_init)\n",
    "        return tf.matmul(x, w)+b\n",
    "\n",
    "def model(X, M, Y, train=False, reuse=False):\n",
    "    with tf.variable_scope('model', reuse=reuse):\n",
    "        we = tf.get_variable(\"we\", [n_vocab+n_special+n_ctx, n_embd], initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        we = dropout(we, embd_pdrop, train)\n",
    "\n",
    "        X = tf.reshape(X, [-1, n_ctx, 2])\n",
    "        M = tf.reshape(M, [-1, n_ctx])\n",
    "\n",
    "        h = embed(X, we)\n",
    "        for layer in range(n_layer):\n",
    "            h = block(h, 'h%d'%layer, train=train, scale=True)\n",
    "\n",
    "        lm_h = tf.reshape(h[:, :-1], [-1, n_embd])\n",
    "        lm_logits = tf.matmul(lm_h, we, transpose_b=True)\n",
    "        lm_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=lm_logits, labels=tf.reshape(X[:, 1:, 0], [-1]))\n",
    "        lm_losses = tf.reshape(lm_losses, [shape_list(X)[0], shape_list(X)[1]-1])\n",
    "        lm_losses = tf.reduce_sum(lm_losses*M[:, 1:], 1)/tf.reduce_sum(M[:, 1:], 1)\n",
    "\n",
    "        clf_h = tf.reshape(h, [-1, n_embd])\n",
    "        pool_idx = tf.cast(tf.argmax(tf.cast(tf.equal(X[:, :, 0], clf_token), tf.float32), 1), tf.int32)\n",
    "        clf_h = tf.gather(clf_h, tf.range(shape_list(X)[0], dtype=tf.int32)*n_ctx+pool_idx)\n",
    "\n",
    "        clf_h = tf.reshape(clf_h, [-1, 2, n_embd])\n",
    "        if train and clf_pdrop > 0:\n",
    "            shape = shape_list(clf_h)\n",
    "            shape[1] = 1\n",
    "            clf_h = tf.nn.dropout(clf_h, 1-clf_pdrop, shape)\n",
    "        clf_h = tf.reshape(clf_h, [-1, n_embd])\n",
    "        clf_logits = clf(clf_h, 1, train=train)\n",
    "        clf_logits = tf.reshape(clf_logits, [-1, 2])\n",
    "\n",
    "        clf_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=clf_logits, labels=Y)\n",
    "        return clf_logits, clf_losses, lm_losses\n",
    "\n",
    "def mgpu_train(*xs):\n",
    "    gpu_ops = []\n",
    "    gpu_grads = []\n",
    "    xs = (tf.split(x, n_gpu, 0) for x in xs)\n",
    "    for i, xs in enumerate(zip(*xs)):\n",
    "        do_reuse = True if i > 0 else None\n",
    "        with tf.device(assign_to_gpu(i, \"/gpu:0\")), tf.variable_scope(tf.get_variable_scope(), reuse=do_reuse):\n",
    "            clf_logits, clf_losses, lm_losses = model(*xs, train=True, reuse=do_reuse)\n",
    "            if lm_coef > 0:\n",
    "                train_loss = tf.reduce_mean(clf_losses) + lm_coef*tf.reduce_mean(lm_losses)\n",
    "            else:\n",
    "                train_loss = tf.reduce_mean(clf_losses)\n",
    "            params = find_trainable_variables(\"model\")\n",
    "            grads = tf.gradients(train_loss, params)\n",
    "            grads = list(zip(grads, params))\n",
    "            gpu_grads.append(grads)\n",
    "            gpu_ops.append([clf_logits, clf_losses, lm_losses])\n",
    "    ops = [tf.concat(op, 0) for op in zip(*gpu_ops)]\n",
    "    grads = average_grads(gpu_grads)\n",
    "    grads = [g for g, p in grads]\n",
    "    train = opt_fns[opt](params, grads, lr, partial(lr_schedules[lr_schedule], warmup=lr_warmup), n_updates_total, l2=l2, max_grad_norm=max_grad_norm, vector_l2=vector_l2, b1=b1, b2=b2, e=e)\n",
    "    return [train]+ops\n",
    "\n",
    "def mgpu_predict(*xs):\n",
    "    gpu_ops = []\n",
    "    xs = (tf.split(x, n_gpu, 0) for x in xs)\n",
    "    for i, xs in enumerate(zip(*xs)):\n",
    "        with tf.device(assign_to_gpu(i, \"/gpu:0\")), tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "            clf_logits, clf_losses, lm_losses = model(*xs, train=False, reuse=True)\n",
    "            gpu_ops.append([clf_logits, clf_losses, lm_losses])\n",
    "    ops = [tf.concat(op, 0) for op in zip(*gpu_ops)]\n",
    "    return ops\n",
    "\n",
    "def transform_roc(X1, X2):\n",
    "    n_batch = len(X1)\n",
    "    xmb = np.zeros((n_batch, 2, n_ctx, 2), dtype=np.int32)\n",
    "    mmb = np.zeros((n_batch, 2, n_ctx), dtype=np.float32)\n",
    "    start = encoder['_start_']\n",
    "    delimiter = encoder['_delimiter_']\n",
    "    for i, (x1, x2), in enumerate(zip(X1, X2)):\n",
    "        x12 = [start]+x1[:max_len]+[delimiter]+x2[:max_len]+[clf_token]\n",
    "        x13 = [start]+x2[:max_len]+[delimiter]+x1[:max_len]+[clf_token]\n",
    "        l12 = len(x12)\n",
    "        l13 = len(x13)\n",
    "        xmb[i, 0, :l12, 0] = x12\n",
    "        xmb[i, 1, :l13, 0] = x13\n",
    "        mmb[i, 0, :l12] = 1\n",
    "        mmb[i, 1, :l13] = 1\n",
    "    xmb[:, :, :, 1] = np.arange(n_vocab+n_special, n_vocab+n_special+n_ctx)\n",
    "    return xmb, mmb\n",
    "\n",
    "def iter_apply(Xs, Ms, Ys):\n",
    "    fns = [lambda x:np.concatenate(x, 0), lambda x:float(np.sum(x))]\n",
    "    results = []\n",
    "    for xmb, mmb, ymb in iter_data(Xs, Ms, Ys, n_batch=n_batch_train, truncate=False, verbose=True):\n",
    "        n = len(xmb)\n",
    "        if n == n_batch_train:\n",
    "            res = sess.run([eval_mgpu_logits, eval_mgpu_clf_loss], {X_train:xmb, M_train:mmb, Y_train:ymb})\n",
    "        else:\n",
    "            res = sess.run([eval_logits, eval_clf_loss], {X:xmb, M:mmb, Y:ymb})\n",
    "        res = [r*n for r in res]\n",
    "        results.append(res)\n",
    "    results = zip(*results)\n",
    "    return [fn(res) for res, fn in zip(results, fns)]\n",
    "\n",
    "def iter_predict(Xs, Ms):\n",
    "    logits = []\n",
    "    #print(Xs.shape, Ms.shape)\n",
    "    #print(\"n_batch_train\", n_batch_train)\n",
    "    #sys.exit()\n",
    "    for xmb, mmb in iter_data(Xs, Ms, n_batch=n_batch_train, truncate=False, verbose=True):\n",
    "        n = len(xmb)\n",
    "        #print(\"shapes \", xmb.shape, mmb.shape)\n",
    "        #print(eval_mgpu_logits)\n",
    "        #print(X_train.shape, M_train.shape)\n",
    "        if n == n_batch_train:\n",
    "            logits.append(sess.run(eval_mgpu_logits, {X_train:xmb, M_train:mmb}))\n",
    "        else:\n",
    "            logits.append(sess.run(eval_logits, {X:xmb, M:mmb}))\n",
    "    logits = np.concatenate(logits, 0)\n",
    "    return logits\n",
    "\n",
    "def save(path):\n",
    "    ps = sess.run(params)\n",
    "    joblib.dump(ps, make_path(path))\n",
    "\n",
    "def log():\n",
    "    global best_score\n",
    "    tr_logits, tr_cost = iter_apply(trX[:n_valid], trM[:n_valid], trY[:n_valid])\n",
    "    va_logits, va_cost = iter_apply(vaX, vaM, vaY)\n",
    "    tr_cost = tr_cost/len(trY[:n_valid])\n",
    "    va_cost = va_cost/n_valid\n",
    "    tr_acc = accuracy_score(trY[:n_valid], np.argmax(tr_logits, 1))*100.\n",
    "    va_acc = accuracy_score(vaY, np.argmax(va_logits, 1))*100.\n",
    "    logger.log(n_epochs=n_epochs, n_updates=n_updates, tr_cost=tr_cost, va_cost=va_cost, tr_acc=tr_acc, va_acc=va_acc)\n",
    "    print('%d %d %.3f %.3f %.2f %.2f'%(n_epochs, n_updates, tr_cost, va_cost, tr_acc, va_acc))\n",
    "    if submit:\n",
    "        score = va_acc\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            save(os.path.join(save_dir, desc, 'best_params.jl'))\n",
    "\n",
    "def ed(*splits, encoder):\n",
    "    encoded_splits = []\n",
    "    for split in splits[0]:\n",
    "        fields = []\n",
    "        #print(\"Yes\")\n",
    "        #for s in split:\n",
    "            #print(s)\n",
    "            #sys.exit()\n",
    "        for field in split:\n",
    "            if isinstance(field[0], str):\n",
    "                field = encoder.encode(field)\n",
    "            fields.append(field)\n",
    "        encoded_splits.append(fields)\n",
    "    return encoded_splits\n",
    "\n",
    "def _rc(path):\n",
    "    with open(path) as f:\n",
    "        f = csv.reader(f)\n",
    "        st = []\n",
    "        ct1 = []\n",
    "        ct2 = []\n",
    "        y = []\n",
    "        for i, line in enumerate(tqdm(list(f), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                s = ''.join(line[1])\n",
    "                c1 = line[2]\n",
    "                st.append(s)\n",
    "                ct1.append(c1)\n",
    "                #print(i, \" \", line[3])\n",
    "                #y.append(int(line[3]))\n",
    "                y.append(0)\n",
    "                #print(st, ct1, y)\n",
    "                #sys.exit()\n",
    "        return st, ct1, y\n",
    "\n",
    "def rc(data_dir, n_train=91, n_valid=10):\n",
    "    event1, event2, y = _rc(os.path.join(data_dir, train_dataset))\n",
    "    teX1, teX2, _ = _rc(os.path.join(data_dir, test_dataset))\n",
    "    tr_event1, va_event1, tr_event2, va_event2, tr_y, va_y = train_test_split(event1, event2, y, test_size=n_valid, random_state=seed)\n",
    "    trX1, trX2, trX3 = [], [], []\n",
    "    trY = []\n",
    "    for s, c1, y in zip(tr_event1, tr_event2, tr_y):\n",
    "        trX1.append(s)\n",
    "        trX2.append(c1)\n",
    "        trY.append(y)\n",
    "\n",
    "    vaX1, vaX2, vaX3 = [], [], []\n",
    "    vaY = []\n",
    "    for s, c1, y in zip(va_event1, va_event1, va_y):\n",
    "        vaX1.append(s)\n",
    "        vaX2.append(c1)\n",
    "        vaY.append(y)\n",
    "        \n",
    "    trY = np.asarray(trY, dtype=np.int32)\n",
    "    vaY = np.asarray(vaY, dtype=np.int32)\n",
    "    return (trX1, trX2, trY), (vaX1, vaX2, vaY), (teX1, teX2)\n",
    "\n",
    "def analyz(data_dir, pred_path, log_path):\n",
    "    import pandas as pd\n",
    "    preds = pd.read_csv(pred_path, delimiter='\\t')['prediction'].values.tolist()\n",
    "    _, _, labels = _rc(os.path.join(data_dir, test_dataset))\n",
    "    test_accuracy = accuracy_score(labels, preds)*100.\n",
    "    #logs = [json.loads(line) for line in open(log_path)][1:]\n",
    "    #best_validation_index = np.argmax([log['va_acc'] for log in logs])\n",
    "    #valid_accuracy = logs[best_validation_index]['va_acc']\n",
    "    #print('Valid Accuracy: %.2f'%(valid_accuracy))\n",
    "    print('Test Accuracy:  %.2f'%(test_accuracy))\n",
    "    y_pred = np.array(preds)\n",
    "    y_true = np.array(labels)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "    con_mat = confusion_matrix(y_true, y_pred)\n",
    "    print(con_mat)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "def predict_(dataset_):\n",
    "    filename = filenames[dataset_]\n",
    "    pred_fn = pred_fns[dataset_]\n",
    "    label_decoder = label_decoders[dataset_]\n",
    "    predictions = pred_fn(iter_predict(teX, teM))\n",
    "    if label_decoder is not None:\n",
    "        predictions = [label_decoder[prediction] for prediction in predictions]\n",
    "    path = os.path.join(submission_dir, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    print(len(predictions))\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('{}\\t{}\\n'.format('index', 'prediction'))\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            f.write('{}\\t{}\\n'.format(i, prediction))\n",
    "    return predictions\n",
    "            \n",
    "\n",
    "argmax = lambda x:np.argmax(x, 1)\n",
    "\n",
    "pred_fns = {\n",
    "    'rocstories':argmax,\n",
    "    'eventpair':argmax,\n",
    "    'entailment':argmax\n",
    "}\n",
    "\n",
    "filenames = {\n",
    "    'rocstories':'ROCStories.tsv',\n",
    "    'eventpair':'eventpair.tsv',\n",
    "    'entailment':'entailment.tsv'\n",
    "}\n",
    "\n",
    "label_decoders = {\n",
    "    'rocstories':None,\n",
    "    'eventpair':None,\n",
    "    'entailment':None\n",
    "}\n",
    "            \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--desc', type=str, default='entailment')\n",
    "parser.add_argument('--dataset', type=str, default='entailment')\n",
    "parser.add_argument('--log_dir', type=str, default='classification_model/log/')\n",
    "parser.add_argument('--save_dir', type=str, default='classification_model/save/')\n",
    "parser.add_argument('--data_dir', type=str, default='data')\n",
    "parser.add_argument('--submission_dir', type=str, default='classification_model/submission/')\n",
    "parser.add_argument('--submit', action='store_true', default=True)\n",
    "parser.add_argument('--analysis', action='store_true', default=True)\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "parser.add_argument('--n_iter', type=int, default=3)\n",
    "parser.add_argument('--n_batch', type=int, default=8)\n",
    "parser.add_argument('--max_grad_norm', type=int, default=1)\n",
    "parser.add_argument('--lr', type=float, default=6.25e-5)\n",
    "parser.add_argument('--lr_warmup', type=float, default=0.002)\n",
    "parser.add_argument('--n_ctx', type=int, default=512)\n",
    "parser.add_argument('--n_embd', type=int, default=768)\n",
    "parser.add_argument('--n_head', type=int, default=12)\n",
    "parser.add_argument('--n_layer', type=int, default=12)\n",
    "parser.add_argument('--embd_pdrop', type=float, default=0.1)\n",
    "parser.add_argument('--attn_pdrop', type=float, default=0.1)\n",
    "parser.add_argument('--resid_pdrop', type=float, default=0.1)\n",
    "parser.add_argument('--clf_pdrop', type=float, default=0.1)\n",
    "parser.add_argument('--l2', type=float, default=0.01)\n",
    "parser.add_argument('--vector_l2', action='store_true')\n",
    "parser.add_argument('--n_gpu', type=int, default=4)\n",
    "parser.add_argument('--opt', type=str, default='adam')\n",
    "parser.add_argument('--afn', type=str, default='gelu')\n",
    "parser.add_argument('--lr_schedule', type=str, default='warmup_linear')\n",
    "parser.add_argument('--encoder_path', type=str, default='classification_model/model/encoder_bpe_40000.json')\n",
    "parser.add_argument('--bpe_path', type=str, default='classification_model/model/vocab_40000.bpe')\n",
    "parser.add_argument('--n_transfer', type=int, default=12)\n",
    "parser.add_argument('--lm_coef', type=float, default=0.5)\n",
    "parser.add_argument('--b1', type=float, default=0.9)\n",
    "parser.add_argument('--b2', type=float, default=0.999)\n",
    "parser.add_argument('--e', type=float, default=1e-8)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "#print(args)\n",
    "globals().update(args.__dict__)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "logger = ResultLogger(path=os.path.join(log_dir, '{}.jsonl'.format(desc)), **args.__dict__)\n",
    "text_encoder = TextEncoder(encoder_path, bpe_path)\n",
    "encoder = text_encoder.encoder\n",
    "n_vocab = len(text_encoder.encoder)\n",
    "\n",
    "(trX1, trX2, trY), (vaX1, vaX2, vaY), (teX1, teX2) = ed(rc(data_dir), encoder=text_encoder)\n",
    "n_y = 2\n",
    "encoder['_start_'] = len(encoder)\n",
    "encoder['_delimiter_'] = len(encoder)\n",
    "encoder['_classify_'] = len(encoder)\n",
    "clf_token = encoder['_classify_']\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "max_len = n_ctx//2-2\n",
    "imported_meta = tf.train.import_meta_graph(\"saved_nodel/my_test_model.meta\")\n",
    "l1_ = [len(x1[:max_len])+len(x2[:max_len]) for x1, x2 in zip(trX1, trX2)]\n",
    "l2_ = [len(x1[:max_len])+len(x2[:max_len]) for x1, x2 in zip(vaX1, vaX2)]\n",
    "l3_ = [len(x1[:max_len])+len(x2[:max_len]) for x1, x2 in zip(teX1, teX2)]\n",
    "\n",
    "n_ctx = min(max(l1_+l2_+l3_)+3, n_ctx)\n",
    "n_ctx = 256\n",
    "n_special = 3\n",
    "n_batch_train = n_batch*n_gpu\n",
    "\n",
    "trX, trM = transform_roc(trX1, trX2)\n",
    "vaX, vaM = transform_roc(vaX1, vaX2)\n",
    "#if submit:\n",
    "teX, teM = transform_roc(teX1, teX2)\n",
    "\n",
    "n_train = len(trY)\n",
    "n_valid = len(vaY)\n",
    "n_batch_train = n_batch*n_gpu\n",
    "n_updates_total = (n_train//n_batch_train)*n_iter\n",
    "\n",
    "X_train = tf.placeholder(tf.int32, [n_batch_train, 2, n_ctx, 2])\n",
    "M_train = tf.placeholder(tf.float32, [n_batch_train, 2, n_ctx])\n",
    "X = tf.placeholder(tf.int32, [None, 2, n_ctx, 2])\n",
    "M = tf.placeholder(tf.float32, [None, 2, n_ctx])\n",
    "\n",
    "Y_train = tf.placeholder(tf.int32, [n_batch_train])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "print(len(trX1), len(trX2), len(trY), len(vaX1), len(vaX2), len(vaY), len(teX1), len(teX2))\n",
    "print(encoder['_delimiter_'])\n",
    "print(n_ctx)\n",
    "print(max_len)\n",
    "\n",
    "print(trX.shape, trM.shape, vaX.shape, vaM.shape, teX.shape, teM.shape)\n",
    "print(n_train, n_valid, n_batch_train, n_updates_total)\n",
    "\n",
    "print(X.shape)\n",
    "print(M.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(M_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "WARNING:tensorflow:From /home/ahaque2/project/virtual_environment_1/NLP_791_Project/final_code/classification_model/utils.py:129: The name tf.NodeDef is deprecated. Please use tf.compat.v1.NodeDef instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-1eb44d91c976>:40: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ahaque2/project/virtual_environment_1/NLP_791_Project/final_code/classification_model/utils.py:92: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ahaque2/project/virtual_environment_1/NLP_791_Project/final_code/classification_model/utils.py:92: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ahaque2/project/virtual_environment_1/lib/python3.5/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "can't train model/we:0 None\n",
      "can't train model/h0/attn/c_attn/w:0 None\n",
      "can't train model/h0/attn/c_attn/b:0 None\n",
      "can't train model/h0/attn/c_proj/w:0 None\n",
      "can't train model/h0/attn/c_proj/b:0 None\n",
      "can't train model/h0/ln_1/g:0 None\n",
      "can't train model/h0/ln_1/b:0 None\n",
      "can't train model/h0/mlp/c_fc/w:0 None\n",
      "can't train model/h0/mlp/c_fc/b:0 None\n",
      "can't train model/h0/mlp/c_proj/w:0 None\n",
      "can't train model/h0/mlp/c_proj/b:0 None\n",
      "can't train model/h0/ln_2/g:0 None\n",
      "can't train model/h0/ln_2/b:0 None\n",
      "can't train model/h1/attn/c_attn/w:0 None\n",
      "can't train model/h1/attn/c_attn/b:0 None\n",
      "can't train model/h1/attn/c_proj/w:0 None\n",
      "can't train model/h1/attn/c_proj/b:0 None\n",
      "can't train model/h1/ln_1/g:0 None\n",
      "can't train model/h1/ln_1/b:0 None\n",
      "can't train model/h1/mlp/c_fc/w:0 None\n",
      "can't train model/h1/mlp/c_fc/b:0 None\n",
      "can't train model/h1/mlp/c_proj/w:0 None\n",
      "can't train model/h1/mlp/c_proj/b:0 None\n",
      "can't train model/h1/ln_2/g:0 None\n",
      "can't train model/h1/ln_2/b:0 None\n",
      "can't train model/h2/attn/c_attn/w:0 None\n",
      "can't train model/h2/attn/c_attn/b:0 None\n",
      "can't train model/h2/attn/c_proj/w:0 None\n",
      "can't train model/h2/attn/c_proj/b:0 None\n",
      "can't train model/h2/ln_1/g:0 None\n",
      "can't train model/h2/ln_1/b:0 None\n",
      "can't train model/h2/mlp/c_fc/w:0 None\n",
      "can't train model/h2/mlp/c_fc/b:0 None\n",
      "can't train model/h2/mlp/c_proj/w:0 None\n",
      "can't train model/h2/mlp/c_proj/b:0 None\n",
      "can't train model/h2/ln_2/g:0 None\n",
      "can't train model/h2/ln_2/b:0 None\n",
      "can't train model/h3/attn/c_attn/w:0 None\n",
      "can't train model/h3/attn/c_attn/b:0 None\n",
      "can't train model/h3/attn/c_proj/w:0 None\n",
      "can't train model/h3/attn/c_proj/b:0 None\n",
      "can't train model/h3/ln_1/g:0 None\n",
      "can't train model/h3/ln_1/b:0 None\n",
      "can't train model/h3/mlp/c_fc/w:0 None\n",
      "can't train model/h3/mlp/c_fc/b:0 None\n",
      "can't train model/h3/mlp/c_proj/w:0 None\n",
      "can't train model/h3/mlp/c_proj/b:0 None\n",
      "can't train model/h3/ln_2/g:0 None\n",
      "can't train model/h3/ln_2/b:0 None\n",
      "can't train model/h4/attn/c_attn/w:0 None\n",
      "can't train model/h4/attn/c_attn/b:0 None\n",
      "can't train model/h4/attn/c_proj/w:0 None\n",
      "can't train model/h4/attn/c_proj/b:0 None\n",
      "can't train model/h4/ln_1/g:0 None\n",
      "can't train model/h4/ln_1/b:0 None\n",
      "can't train model/h4/mlp/c_fc/w:0 None\n",
      "can't train model/h4/mlp/c_fc/b:0 None\n",
      "can't train model/h4/mlp/c_proj/w:0 None\n",
      "can't train model/h4/mlp/c_proj/b:0 None\n",
      "can't train model/h4/ln_2/g:0 None\n",
      "can't train model/h4/ln_2/b:0 None\n",
      "can't train model/h5/attn/c_attn/w:0 None\n",
      "can't train model/h5/attn/c_attn/b:0 None\n",
      "can't train model/h5/attn/c_proj/w:0 None\n",
      "can't train model/h5/attn/c_proj/b:0 None\n",
      "can't train model/h5/ln_1/g:0 None\n",
      "can't train model/h5/ln_1/b:0 None\n",
      "can't train model/h5/mlp/c_fc/w:0 None\n",
      "can't train model/h5/mlp/c_fc/b:0 None\n",
      "can't train model/h5/mlp/c_proj/w:0 None\n",
      "can't train model/h5/mlp/c_proj/b:0 None\n",
      "can't train model/h5/ln_2/g:0 None\n",
      "can't train model/h5/ln_2/b:0 None\n",
      "can't train model/h6/attn/c_attn/w:0 None\n",
      "can't train model/h6/attn/c_attn/b:0 None\n",
      "can't train model/h6/attn/c_proj/w:0 None\n",
      "can't train model/h6/attn/c_proj/b:0 None\n",
      "can't train model/h6/ln_1/g:0 None\n",
      "can't train model/h6/ln_1/b:0 None\n",
      "can't train model/h6/mlp/c_fc/w:0 None\n",
      "can't train model/h6/mlp/c_fc/b:0 None\n",
      "can't train model/h6/mlp/c_proj/w:0 None\n",
      "can't train model/h6/mlp/c_proj/b:0 None\n",
      "can't train model/h6/ln_2/g:0 None\n",
      "can't train model/h6/ln_2/b:0 None\n",
      "can't train model/h7/attn/c_attn/w:0 None\n",
      "can't train model/h7/attn/c_attn/b:0 None\n",
      "can't train model/h7/attn/c_proj/w:0 None\n",
      "can't train model/h7/attn/c_proj/b:0 None\n",
      "can't train model/h7/ln_1/g:0 None\n",
      "can't train model/h7/ln_1/b:0 None\n",
      "can't train model/h7/mlp/c_fc/w:0 None\n",
      "can't train model/h7/mlp/c_fc/b:0 None\n",
      "can't train model/h7/mlp/c_proj/w:0 None\n",
      "can't train model/h7/mlp/c_proj/b:0 None\n",
      "can't train model/h7/ln_2/g:0 None\n",
      "can't train model/h7/ln_2/b:0 None\n",
      "can't train model/h8/attn/c_attn/w:0 None\n",
      "can't train model/h8/attn/c_attn/b:0 None\n",
      "can't train model/h8/attn/c_proj/w:0 None\n",
      "can't train model/h8/attn/c_proj/b:0 None\n",
      "can't train model/h8/ln_1/g:0 None\n",
      "can't train model/h8/ln_1/b:0 None\n",
      "can't train model/h8/mlp/c_fc/w:0 None\n",
      "can't train model/h8/mlp/c_fc/b:0 None\n",
      "can't train model/h8/mlp/c_proj/w:0 None\n",
      "can't train model/h8/mlp/c_proj/b:0 None\n",
      "can't train model/h8/ln_2/g:0 None\n",
      "can't train model/h8/ln_2/b:0 None\n",
      "can't train model/h9/attn/c_attn/w:0 None\n",
      "can't train model/h9/attn/c_attn/b:0 None\n",
      "can't train model/h9/attn/c_proj/w:0 None\n",
      "can't train model/h9/attn/c_proj/b:0 None\n",
      "can't train model/h9/ln_1/g:0 None\n",
      "can't train model/h9/ln_1/b:0 None\n",
      "can't train model/h9/mlp/c_fc/w:0 None\n",
      "can't train model/h9/mlp/c_fc/b:0 None\n",
      "can't train model/h9/mlp/c_proj/w:0 None\n",
      "can't train model/h9/mlp/c_proj/b:0 None\n",
      "can't train model/h9/ln_2/g:0 None\n",
      "can't train model/h9/ln_2/b:0 None\n",
      "can't train model/h10/attn/c_attn/w:0 None\n",
      "can't train model/h10/attn/c_attn/b:0 None\n",
      "can't train model/h10/attn/c_proj/w:0 None\n",
      "can't train model/h10/attn/c_proj/b:0 None\n",
      "can't train model/h10/ln_1/g:0 None\n",
      "can't train model/h10/ln_1/b:0 None\n",
      "can't train model/h10/mlp/c_fc/w:0 None\n",
      "can't train model/h10/mlp/c_fc/b:0 None\n",
      "can't train model/h10/mlp/c_proj/w:0 None\n",
      "can't train model/h10/mlp/c_proj/b:0 None\n",
      "can't train model/h10/ln_2/g:0 None\n",
      "can't train model/h10/ln_2/b:0 None\n",
      "can't train model/h11/attn/c_attn/w:0 None\n",
      "can't train model/h11/attn/c_attn/b:0 None\n",
      "can't train model/h11/attn/c_proj/w:0 None\n",
      "can't train model/h11/attn/c_proj/b:0 None\n",
      "can't train model/h11/ln_1/g:0 None\n",
      "can't train model/h11/ln_1/b:0 None\n",
      "can't train model/h11/mlp/c_fc/w:0 None\n",
      "can't train model/h11/mlp/c_fc/b:0 None\n",
      "can't train model/h11/mlp/c_proj/w:0 None\n",
      "can't train model/h11/mlp/c_proj/b:0 None\n",
      "can't train model/h11/ln_2/g:0 None\n",
      "can't train model/h11/ln_2/b:0 None\n",
      "can't train model/clf/w:0 None\n",
      "can't train model/clf/b:0 None\n",
      "WARNING:tensorflow:From /home/ahaque2/project/virtual_environment_1/lib/python3.5/site-packages/tensorflow_core/python/ops/variables.py:2825: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "1\n",
      "2  294 <tf.Variable 'model/we:0' shape=(40737, 768) dtype=float32_ref>\n",
      "3\n",
      "4\n",
      "40737\n",
      "This is right Here\n",
      "294 <tf.Variable 'model/we:0' shape=(40737, 768) dtype=float32_ref> 145 [[ 0.04640824 -0.12804979 -0.01534251 ... -0.13702106  0.08278882\n",
      "  -0.08590179]\n",
      " [ 0.0761694  -0.08935219  0.07986864 ... -0.05930787  0.10341352\n",
      "  -0.15064521]\n",
      " [-0.06938171 -0.09637386 -0.04659738 ... -0.10917546  0.08646749\n",
      "  -0.11113402]\n",
      " ...\n",
      " [ 0.02837771  0.00309996 -0.00211529 ... -0.02920505 -0.0098196\n",
      "   0.01442775]\n",
      " [ 0.0317276  -0.00358358 -0.00386328 ... -0.04132427 -0.01348734\n",
      "   0.0209565 ]\n",
      " [ 0.02782451 -0.00178694 -0.0056343  ... -0.04545041 -0.02463073\n",
      "   0.04811706]]\n",
      "Now here we are \n",
      "294 <tf.Variable 'model/we:0' shape=(40737, 768) dtype=float32_ref>\n",
      "Now HERRRRRE\n"
     ]
    }
   ],
   "source": [
    "print(\"0\")\n",
    "train, logits, clf_losses, lm_losses = mgpu_train(X_train, M_train, Y_train)\n",
    "clf_loss = tf.reduce_mean(clf_losses)\n",
    "print(\"1\")\n",
    "params = find_trainable_variables('model')\n",
    "print(\"2 \", len(params), params[0])\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "#imported_meta.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "print(\"3\")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"4\")\n",
    "shapes = json.load(open('classification_model/model/params_shapes.json'))\n",
    "offsets = np.cumsum([np.prod(shape) for shape in shapes])\n",
    "init_params = [np.load('classification_model/model/params_{}.npy'.format(n)) for n in range(10)]\n",
    "init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n",
    "init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n",
    "init_params[0] = init_params[0][:n_ctx]\n",
    "init_params[0] = np.concatenate([init_params[1], (np.random.randn(n_special, n_embd)*0.02).astype(np.float32), init_params[0]], 0)\n",
    "print(len(init_params[0]))\n",
    "del init_params[1]\n",
    "print(\"This is right Here\")\n",
    "print(len(params), params[0], len(init_params), init_params[0])\n",
    "sess.run([p.assign(ip) for p, ip in zip(params[:n_transfer], init_params[:n_transfer])])\n",
    "print(\"Now here we are \")\n",
    "print(len(params), params[0])\n",
    "sess.run([p.assign(ip) for p, ip in zip(params, joblib.load(os.path.join(save_dir, desc, 'best_params.jl')))])\n",
    "print(\"Now HERRRRRE\")\n",
    "\n",
    "eval_mgpu_logits, eval_mgpu_clf_losses, eval_mgpu_lm_losses = mgpu_predict(X_train, M_train, Y_train)\n",
    "eval_logits, eval_clf_losses, eval_lm_losses = model(X, M, Y, train=False, reuse=True)\n",
    "eval_clf_loss = tf.reduce_mean(eval_clf_losses)\n",
    "eval_mgpu_clf_loss = tf.reduce_mean(eval_mgpu_clf_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predic_(dataset_):\n",
    "    filename = filenames[dataset_]\n",
    "    pred_fn = pred_fns[dataset_]\n",
    "    label_decoder = label_decoders[dataset_]\n",
    "    iter_pred = iter_predict(teX, teM)\n",
    "    print(iter_pred)\n",
    "    #sys.exit()\n",
    "    predictions = pred_fn(iter_pred)\n",
    "    if label_decoder is not None:\n",
    "        predictions = [label_decoder[prediction] for prediction in predictions]\n",
    "    path = os.path.join(submission_dir, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    print(len(predictions))\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('{}\\t{}\\n'.format('index', 'prediction'))\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            f.write('{}\\t{}\\n'.format(i, prediction))\n",
    "    return predictions, iter_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55000126 -0.4926156 ]\n",
      " [ 0.07413381  0.02719757]\n",
      " [-0.19589825 -0.16819851]\n",
      " ...\n",
      " [-0.811062   -0.7719482 ]\n",
      " [ 0.06596909  0.08129586]\n",
      " [ 0.00469982  0.0413203 ]]\n",
      "4086\n",
      "(4086,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#sess.run([p.assign(ip) for p, ip in zip(params, joblib.load(os.path.join(save_dir, desc, 'best_params.jl')))])\n",
    "pred, iter_pred = predic_('entailment')\n",
    "print(pred.shape)\n",
    "#analyz(data_dir, os.path.join(submission_dir, 'entailment.tsv'), os.path.join(log_dir, 'entailment.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(iter_pred):\n",
    "    diff = [abs(i[0] - i[1]) for i in iter_pred]\n",
    "    #print(diff)\n",
    "    pred_labels = []\n",
    "    diff = np.array(diff)\n",
    "    index = np.where(diff > 0.10)[0]\n",
    "    for d, row in zip(diff, iter_pred):\n",
    "        if(d>0.1):\n",
    "            if(row[0] < row[1]):\n",
    "                pred_labels.append(1)\n",
    "            else:\n",
    "                pred_labels.append(0)\n",
    "        else:\n",
    "            pred_labels.append(0)\n",
    "\n",
    "\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    index = np.where(pred_labels == 1)[0]\n",
    "    return pred_labels\n",
    "\n",
    "    #print(index, diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6         6\n",
      "104     117\n",
      "106     119\n",
      "110     123\n",
      "111     124\n",
      "113     126\n",
      "220      38\n",
      "222      40\n",
      "225      43\n",
      "263      94\n",
      "264      95\n",
      "313     144\n",
      "316     147\n",
      "326     157\n",
      "355     199\n",
      "373       9\n",
      "392      28\n",
      "395      31\n",
      "413      50\n",
      "456      94\n",
      "563     203\n",
      "572     212\n",
      "579     264\n",
      "585     270\n",
      "603     288\n",
      "607     292\n",
      "714     402\n",
      "717     405\n",
      "720     408\n",
      "732     420\n",
      "       ... \n",
      "3759    103\n",
      "3780    124\n",
      "3782    126\n",
      "3808    152\n",
      "3812    156\n",
      "3827    171\n",
      "3897    241\n",
      "3898    242\n",
      "3899    243\n",
      "3900    244\n",
      "3901    245\n",
      "3904    248\n",
      "3930    274\n",
      "3936    280\n",
      "3937    281\n",
      "3938    282\n",
      "3939    283\n",
      "3940    284\n",
      "3942    286\n",
      "3943    287\n",
      "3945    289\n",
      "3968    312\n",
      "3970    314\n",
      "3972    316\n",
      "3979    323\n",
      "3980    324\n",
      "4009    353\n",
      "4019    363\n",
      "4047    401\n",
      "4079    433\n",
      "Name: sent_id, Length: 146, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv('data/cluster_0_news_unprocessed.csv')\n",
    "d = d.iloc[index]\n",
    "print(d['sent_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4086,)\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)\n",
    "d['pred'] = pred\n",
    "d.to_csv('data/cluster_0_news_unprocessed_with_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 1]\n",
      "[0 1] [2043 2043]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(pred, return_counts=True)\n",
    "print(pred)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
